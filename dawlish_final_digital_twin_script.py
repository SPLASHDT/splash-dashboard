# -*- coding: utf-8 -*-
"""Untitled24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bdus3Ws468971od4zDu5q_Y8w7Z9tEDu
"""

# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oi3eiWvFL0IOZTk2De8TBzSevCAWblMi
"""

"""SPLASH Digital Twin Dawlish"""

# Script for using the different pretrained machine learning models to predict (1) wave overtopping occurrences and (2) overtopping frequency.
# - Inputs are: Forecasting data (tidal, wind, wave).
# 6 input features:

# 1.- VHM/Hs (significant Wave Height)
# 2.- VTM02/Tm (Mean Period)
# 3.- VMDR/shoreWaveDir (direction of the waves)
# 4.- water_level/Freeboard (how high the water level is)
# 5.- Wind Direction/shoreWindDir (just the wind direction)
# 6.- Wind Speed/Wind(m/s) (wind speed)

# your final dataset when concatenating all this dataset will have all these variables. 

# - Outputs are: Digital Twin interface.

# Authors: Michael McGlade, Nieves G. Valiente, Jennifer Brown, Christopher Stokes, Timothy Poate




# Step 1: Import necessary libraries
import matplotlib.pyplot as plt
import pandas as pd
import joblib
import ipywidgets as widgets
from IPython.display import display, clear_output
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from datetime import datetime
import xarray as xr
import matplotlib.lines as mlines
# !pip install pygrib
import pygrib
import numpy as np
from datetime import datetime, timedelta


plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.serif'] = ['DejaVu Serif']




# Step 2: Load Datasets
# from google.colab import drive
# drive.mount('/content/drive', force_remount=True)



# Wave (Dataset 1)
Dawlish_Wave_Buoy_latitude = 50.56757 # this is our Dawlish Buoy coordinates
Dawlish_Wave_Buoy_longitude = -3.42424
# wvl_files_path = '/content/drive/MyDrive/'
wvl_files_path = './assets/datasets/wave_level/'
wave_file_paths_we_need = [
    wvl_files_path + 'metoffice_wave_amm15_NWS_WAV_b20241104_hi20241104.nc',  #these are netCDF files
    wvl_files_path + 'metoffice_wave_amm15_NWS_WAV_b20241104_hi20241105.nc',
    wvl_files_path + 'metoffice_wave_amm15_NWS_WAV_b20241104_hi20241106.nc',
    wvl_files_path + 'metoffice_wave_amm15_NWS_WAV_b20241104_hi20241107.nc',
    wvl_files_path + 'metoffice_wave_amm15_NWS_WAV_b20241104_hi20241108.nc',
    wvl_files_path + 'metoffice_wave_amm15_NWS_WAV_b20241104_hi20241109.nc'
]
Our_Extracted_Variables = ['time', 'VHM0', 'VTM02', 'VMDR'] #these are the only varibles we must extract
df_list = []
for file_path in wave_file_paths_we_need:
    ds_wave_Dawlishy = xr.open_dataset(file_path)
    ds_filtered_wave_Dawlish = ds_wave_Dawlishy.sel(latitude=Dawlish_Wave_Buoy_latitude, longitude=Dawlish_Wave_Buoy_longitude, method="nearest")[Our_Extracted_Variables] # we apply Ecludian distance to geolocate the closest coordinates to the predefined wave buoy coordinates
    ds_wave_Dawlishy = ds_filtered_wave_Dawlish.to_dataframe().reset_index()
    ds_wave_Dawlishy = ds_wave_Dawlishy.rename(columns={'time': 'datetime', 'VHM0': 'Hs', 'VTM02': 'Tm', 'VMDR': 'shoreWaveDir'})  #We convert the variable names to our model training variable names, sorry for confusion
    ds_wave_Dawlishy = ds_wave_Dawlishy[['datetime', 'Hs', 'Tm', 'shoreWaveDir', 'latitude', 'longitude']]
    ds_wave_Dawlishy['datetime'] = pd.to_datetime(ds_wave_Dawlishy['datetime'])
    df_list.append(ds_wave_Dawlishy)
combined_df = pd.concat(df_list, ignore_index=True)
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)



# Water Level (Dataset 2)
# wl_dataset_path = '/content/'
wl_dataset_path = './assets/datasets/water_level/'
NOC_SPLASH_Water_Level_Data = pd.read_csv(
    wl_dataset_path + 'EXMOUTH Jan 22 to Dec 26 (2).txt', # Now we must extract our water lebvels from the text file
    sep='\s+',
    header=None,
    skiprows=2,
    names=['date', 'time', 'water_level'], # these arew the only variables we need
    engine='python'
)
NOC_SPLASH_Water_Level_Data['datetime'] = pd.to_datetime(NOC_SPLASH_Water_Level_Data['date'] + ' ' + NOC_SPLASH_Water_Level_Data['time'], format='%d/%m/%Y %H:%M')
NOC_SPLASH_Water_Level_Data = NOC_SPLASH_Water_Level_Data.set_index('datetime')[['water_level']]

if 'datetime' not in combined_df.columns:
    combined_df = combined_df.reset_index()
combined_df = combined_df.set_index('datetime')

Now_combine_with_waterlevel = combined_df.join(NOC_SPLASH_Water_Level_Data, how='left')
Now_combine_with_waterlevel = Now_combine_with_waterlevel[['Hs', 'Tm', 'shoreWaveDir', 'latitude', 'longitude', 'water_level']] # combine are wave parameters with the water levels



# Wind Speed (Dataset 3)
# ws_files_path = '/content/'
ws_files_path = './assets/datasets/wind/'
Next_download_wind_values = ws_files_path + 'agl_wind-speed-surface-adjusted_10.0_+00 (2) (1).grib2'
Wave_Buoy_lat = 50.56757 # again our wave buoy lat/long values
Wave_Buoy_lon = -3.42424
data = []
grbs = pygrib.open(Next_download_wind_values)
for grb in grbs:
    if grb.level == 10: # means 10 m above landsurface
        values_for_concatinating_speed = grb.values
        lats, lons = grb.latlons()
        next_convert_long = [(lon if lon <= 180 else lon - 360) for lon in lons.flatten()] # making the coordinate identification system easier to understand and concatenate with our origional dataframes
        distances = np.sqrt((lats - Wave_Buoy_lat)**2 + (np.array(next_convert_long).reshape(lons.shape) - Wave_Buoy_lon)**2)
        Making_min_distance_index = np.unravel_index(distances.argmin(), distances.shape)
        wind_speed_value = values_for_concatinating_speed[Making_min_distance_index]
        Our_nearest_lat_found_buoy = lats[Making_min_distance_index] # ensures the closes ecludian distance is offshore to the wave buoy.
        Our_nearest_long_found_buoy = next_convert_long[Making_min_distance_index[1]]
        data_date = grb.dataDate
        data_time = grb.dataTime
        forecast_time = grb.forecastTime

        Sort_out_input_dates = datetime.strptime(f"{data_date:08d}{data_time:04d}", "%Y%m%d%H%M")
        forecast_datetime = Sort_out_input_dates + timedelta(hours=forecast_time)
        data.append({
            'Forecast DateTime': forecast_datetime,
            'Latitude': Our_nearest_lat_found_buoy,
            'Longitude': Our_nearest_long_found_buoy,
            'Wind Speed': wind_speed_value
        })

grbs.close()
df = pd.DataFrame(data)

df['datetime'] = pd.to_datetime(df['Forecast DateTime'])
df = df.set_index('datetime')[['Wind Speed']]
df = df.resample('h').interpolate()

combined_df_final = Now_combine_with_waterlevel
combined_df_final = combined_df_final.join(df, how='left')
combined_df_final = combined_df_final[['Hs', 'Tm', 'shoreWaveDir', 'latitude', 'longitude', 'water_level', 'Wind Speed']] # Again we concatenate wind speed with our wave and WL data



# Wind Direction (Dataset 4)

# wind_direction_file_path = '/content/agl_wind-direction-from-which-blowing-surface-adjusted_10.0_+00 (1) (1).grib2'
wind_direction_file_path = './assets/datasets/wind/agl_wind-direction-from-which-blowing-surface-adjusted_10.0_+00 (1) (1).grib2'
nearest_lat_to_buoy = 50.56757
nearest_lon_to_buoy = -3.42424
data = []
grbs = pygrib.open(wind_direction_file_path)

for grb in grbs:
    if grb.level == 10:
        values_for_concatinating_direction = grb.values
        lats, lons = grb.latlons()
        converted_lons_simpler = [(lon if lon <= 180 else lon - 360) for lon in lons.flatten()]

        distances = np.sqrt((lats - nearest_lat_to_buoy)**2 + (np.array(converted_lons_simpler).reshape(lons.shape) - nearest_lon_to_buoy)**2)
        min_distance_thats_tolerated = np.unravel_index(distances.argmin(), distances.shape)

        wind_direction_value = values_for_concatinating_direction[min_distance_thats_tolerated]
        nearest_lat_found = lats[min_distance_thats_tolerated]
        nearest_lon_found = converted_lons_simpler[min_distance_thats_tolerated[1]]

        data_date_in_dataset = grb.dataDate
        data_time = grb.dataTime
        forecast_time = grb.forecastTime

        initial_datetime = datetime.strptime(f"{data_date_in_dataset:08d}{data_time:04d}", "%Y%m%d%H%M")
        forecast_datetime = initial_datetime + timedelta(hours=forecast_time)

        data.append({
            'Forecast DateTime': forecast_datetime,
            'Latitude': nearest_lat_found,
            'Longitude': nearest_lon_found,
            'Wind Direction': wind_direction_value
        })

grbs.close()
df = pd.DataFrame(data)



# Final Input Dataset

df['datetime'] = pd.to_datetime(df['Forecast DateTime'])
df = df.set_index('datetime')[['Wind Direction']]
df = df.resample('h').interpolate() # we interpolate the wind data bacause we dont ahve hourly wind.
combined_df_final = combined_df_final.join(df, how='left')
combined_df_final = combined_df_final[['Hs', 'Tm', 'shoreWaveDir', 'latitude', 'longitude', 'water_level', 'Wind Speed', 'Wind Direction']]

final_dataset = combined_df_final.reset_index()[['datetime', 'Hs', 'Tm', 'shoreWaveDir', 'water_level', 'Wind Speed', 'Wind Direction']]
final_dataset = final_dataset.rename(columns={
    'datetime': 'time',
    'Hs': 'Hs',
    'Tm': 'Tm',
    'shoreWaveDir': 'shoreWaveDir',
    'water_level': 'Freeboard',
    'Wind Speed': 'Wind(m/s)',
    'Wind Direction': 'shoreWindDir'
})

# In this dataset, we have essentially extracted three wave variables from the netcdf files along with the corresponding dates and times. Then we have concatenated the hourly WL data from the
# text file and then finally from our Grib files we have concatenated the wind spped and direction, together we have our final dataset which will then be used as forecast data for our models.



# Step 3: Load the pretrained random forests
machine_learning_models = {
    'RF1': {},
    'RF2': {},
    'RF3': {},
    'RF4': {
        'Regressor': {}
    }
}

# Load each model
# dawlish_rf_path = '/content/'
dawlish_rf_path = './assets/dawlish_models/'
machine_learning_models['RF1']['T24'] = joblib.load(dawlish_rf_path + 'RF1T24') # Binary
machine_learning_models['RF1']['T48'] = joblib.load(dawlish_rf_path + 'RF1T48') # Binary
machine_learning_models['RF1']['T72'] = joblib.load(dawlish_rf_path + 'RF1T72') # Binary

machine_learning_models['RF2']['T24'] = joblib.load(dawlish_rf_path + 'RF2T24') # Regression
machine_learning_models['RF2']['T48'] = joblib.load(dawlish_rf_path + 'RF2T48') # Regression
machine_learning_models['RF2']['T72'] = joblib.load(dawlish_rf_path + 'RF2T72') # Regression

machine_learning_models['RF3']['T24'] = joblib.load(dawlish_rf_path + 'RF3T24') # Binary
machine_learning_models['RF3']['T48'] = joblib.load(dawlish_rf_path + 'RF3T48') # Binary
machine_learning_models['RF3']['T72'] = joblib.load(dawlish_rf_path + 'RF3T72') # Binary

machine_learning_models['RF4']['Regressor']['T24'] = joblib.load(dawlish_rf_path + 'RF4T24R') # Regression
machine_learning_models['RF4']['Regressor']['T48'] = joblib.load(dawlish_rf_path + 'RF4T48R') # Regression
machine_learning_models['RF4']['Regressor']['T72'] = joblib.load(dawlish_rf_path + 'RF4T72R') # Regression





# Step 5: Slider Adjustments for Feature Values
# We want to user to also play around with these pretrained models, where they can adjust the key features for inflencing the predcition and see whether overtopping occures or not.
style = {'description_width': '150px'}
slider_layout_design_with_digital_twin = widgets.Layout(width='400px')
Sig_wave_height_slider_output = widgets.FloatSlider(value=0, min=-100, max=100, step=1, description='Hs (%):', style=style, layout=slider_layout_design_with_digital_twin)
tm_slider = widgets.FloatSlider(value=0, min=-100, max=100, step=1, description='Tm (%):', style=style, layout=slider_layout_design_with_digital_twin)
shore_wave_dir_slider = widgets.FloatSlider(value=0, min=0, max=360, step=1, description='ShoreWaveDir (°):', style=style, layout=slider_layout_design_with_digital_twin)
wind_speed_slider = widgets.FloatSlider(value=0, min=-100, max=100, step=1, description='Wind (m/s) (%):', style=style, layout=slider_layout_design_with_digital_twin)
shore_wind_dir_slider = widgets.FloatSlider(value=0, min=0, max=360, step=1, description='ShoreWindDir (°):', style=style, layout=slider_layout_design_with_digital_twin)
freeboard_slider = widgets.FloatSlider(value=0, min=-100, max=100, step=1, description='Freeboard (%):', style=style, layout=slider_layout_design_with_digital_twin)
submit_button = widgets.Button(description="Submit")




# Step 6: Threhsold Adjustments (Functions)
# In our study, we identify certain feature variable aspects where the ML stuggle and misclassify, we assign these threhsold functions to help our models classify correctly during these narrow threhsold undertainity bands.
# These will not dictate the prediction, rather, tell the models to think more closely about its initial prediction.

# Thresholds for Predictions
rf1_hs_threshold = 1.39
rf1_wind_threshold = 7.71
rf1_wave_dir_min = 49
rf1_wave_dir_max = 97

rf3_hs_threshold = 1.65
rf3_wind_threshold = 8.47
rf3_wave_dir_min = 50
rf3_wave_dir_max = 93

def revise_rf1_prediction(rf1_prediction, row):
    hs_value_sweetspot = row['Hs'] > rf1_hs_threshold
    wind_value_sweetspot = row['Wind(m/s)'] > rf1_wind_threshold
    wave_dir_sweetspot = rf1_wave_dir_min <= row['shoreWaveDir'] <= rf1_wave_dir_max
    return 0 if rf1_prediction == 1 and not (hs_value_sweetspot or wind_value_sweetspot or wave_dir_sweetspot) else rf1_prediction

def revise_rf3_prediction(rf3_prediction, row):
    hs_value_sweetspot = row['Hs'] > rf3_hs_threshold
    wind_value_sweetspot = row['Wind(m/s)'] > rf3_wind_threshold
    wave_dir_sweetspot = rf3_wave_dir_min <= row['shoreWaveDir'] <= rf3_wave_dir_max
    return 0 if rf3_prediction == 1 and not (hs_value_sweetspot or wind_value_sweetspot or wave_dir_sweetspot) else rf3_prediction





# Step 7: Confidence Color Assigning
# Here we want to assign a confidence level for each prediction

def get_confidence_color(confidence):
    try:
        confidence = float(confidence)
        if confidence > 0.8:
            return '#00008B'# this is a colour (dark blue I think)
        elif 0.5 < confidence <= 0.8:
            return '#4682B4'
        else:
            return 'aqua'
    except (ValueError, TypeError):
        return 'gray'

def adjust_features(df): # This means we also want to assign confidence levels when we alter the slider adjustments.
    df_adjusted = df.copy()
    df_adjusted['Hs'] *= (1 + Sig_wave_height_slider_output.value / 100)
    df_adjusted['Tm'] *= (1 + tm_slider.value / 100)
    df_adjusted['shoreWaveDir'] = shore_wave_dir_slider.value
    df_adjusted['Wind(m/s)'] *= (1 + wind_speed_slider.value / 100)
    df_adjusted['shoreWindDir'] = shore_wind_dir_slider.value
    df_adjusted['Freeboard'] *= (1 + freeboard_slider.value / 100)
    return df_adjusted





# Step 8: Model Prediction (the fun part)

def process_wave_overtopping(df_adjusted):
    time_stamps = df_adjusted['time'].dropna()
    overtopping_counts_rf1_rf2 = []
    overtopping_counts_rf3_rf4 = []
    rf1_confidences = [] # confidences for our binary models
    rf3_confidences = []

    for idx, row in df_adjusted.iterrows():
        if pd.isna(row['time']):
            continue

# Ensure we read the correct model
        time_difference = (row['time'] - df_adjusted['time'].iloc[0]).total_seconds() / 3600

        if time_difference < 24:  # we are saying if time = <24 hours then apply T24 models.
            selected_model = 'T24'
        elif 24 <= time_difference < 48: # we are saying if time = 24 - 48 hours then apply T48 models.
            selected_model = 'T48'
        else:
            selected_model = 'T72' # we are saying if time > 72 hours then apply T72 models

        input_data = row[['Hs', 'Tm', 'shoreWaveDir', 'Wind(m/s)', 'shoreWindDir', 'Freeboard']].to_frame().T

# Formulating Predictions

        rf1_model = machine_learning_models['RF1'][selected_model] # For Rig 1 (Binary)
        rf1_prediction = rf1_model.predict(input_data)[0]
        rf1_confidence = rf1_model.predict_proba(input_data)[0][1]
        rf1_confidences.append(rf1_confidence) # % confidence as colour

        final_rf1_prediction_must_use = revise_rf1_prediction(rf1_prediction, row)
        if final_rf1_prediction_must_use == 0:
            overtopping_counts_rf1_rf2.append(0)
        else:
            rf2_model = machine_learning_models['RF2'][selected_model] # For Rig 2 (Regression)
            rf2_prediction = rf2_model.predict(input_data)[0]
            overtopping_counts_rf1_rf2.append(rf2_prediction)

        if final_rf1_prediction_must_use == 1:
            rf3_model = machine_learning_models['RF3'][selected_model] # For Rig 3 (Binary)
            rf3_prediction = rf3_model.predict(input_data)[0]
            rf3_confidence = rf3_model.predict_proba(input_data)[0][1]
            rf3_confidences.append(rf3_confidence)
            final_rf3_prediction = revise_rf3_prediction(rf3_prediction, row)
            if final_rf3_prediction == 0:
                overtopping_counts_rf3_rf4.append(0)
            else:
                rf4_regressor = machine_learning_models['RF4']['Regressor'][selected_model] # For Rig 4 (Regression)
                rf4_prediction = rf4_regressor.predict(input_data)[0]
                overtopping_counts_rf3_rf4.append(min(rf4_prediction, rf2_prediction))
        else:
            overtopping_counts_rf3_rf4.append(0)
            rf3_confidences.append(0)



    # Lets Now Visualise our Outputs (Rig 1)
    clear_output(wait=True)
    
    fig, (axes1_DG_Plot, axes2_DG_Plot) = plt.subplots(1, 2, figsize=(16, 5), dpi=300)

    for i, count in enumerate(overtopping_counts_rf1_rf2):
        if count == 0:
            axes1_DG_Plot.scatter(time_stamps.iloc[i], count, marker='x', color='black', s=100, linewidths=1.5) # the balck "X" means RF1 and/or RF3 decide "no overtopping"
        else:
            color = get_confidence_color(rf1_confidences[i])
            axes1_DG_Plot.scatter(time_stamps.iloc[i], count, marker='o', color=color, s=75, edgecolor='black', linewidth=1)

    axes1_DG_Plot.axhline(y=6, color='black', linestyle='--', linewidth=1, label='25% IQR (6)') # lower IQR line
    axes1_DG_Plot.axhline(y=54, color='black', linestyle='--', linewidth=1, label='75% IQR (54)') # upper IQR line
    axes1_DG_Plot.set_ylim(-10, 120)
    axes1_DG_Plot.set_xlabel('Time', fontsize=10)
    axes1_DG_Plot.set_ylabel('No. of Overtopping Occurrences (Per 10 Mins)', fontsize=10)
    axes1_DG_Plot.set_xticks(time_stamps)
    axes1_DG_Plot.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m-%d %H:%M'))
    axes1_DG_Plot.tick_params(axis='x', rotation=90, labelsize=8)
    axes1_DG_Plot.tick_params(axis='y', labelsize=8)



    # Lets Now Visualise our Outputs (Rig 2)
    for i, count in enumerate(overtopping_counts_rf3_rf4):
        if count == 0:
            axes2_DG_Plot.scatter(time_stamps.iloc[i], count, marker='x', color='black', s=100, linewidths=1.5)
        else:
            color = get_confidence_color(rf3_confidences[i])
            axes2_DG_Plot.scatter(time_stamps.iloc[i], count, marker='o', color=color, s=75, edgecolor='black', linewidth=1)

    axes2_DG_Plot.axhline(y=2, color='black', linestyle='--', linewidth=1, label='25% IQR (2)')
    axes2_DG_Plot.axhline(y=9, color='black', linestyle='--', linewidth=1, label='75% IQR (9)')
    axes2_DG_Plot.set_ylim(-5, 120)
    axes2_DG_Plot.set_xlabel('Time', fontsize=10)
    axes2_DG_Plot.set_ylabel('No. of Overtopping Occurrences (Per 10 Mins)', fontsize=10)
    axes2_DG_Plot.set_xticks(time_stamps)
    axes2_DG_Plot.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m-%d %H:%M'))
    axes2_DG_Plot.tick_params(axis='x', rotation=90, labelsize=8)
    axes2_DG_Plot.tick_params(axis='y', labelsize=8)



    # Plotting our Confidence Estimates (Rig 1 & Rig 2)
    high_confidence_scoring_metrics = mlines.Line2D([], [], color='#00008B', marker='o', linestyle='None', markersize=10, label='High Confidence (> 80%)')
    medium_confidence_scoring_metrics = mlines.Line2D([], [], color='#4682B4', marker='o', linestyle='None', markersize=10, label='Medium Confidence (50-80%)')
    low_confidence_scoring_metrics = mlines.Line2D([], [], color='aqua', marker='o', linestyle='None', markersize=10, label='Low Confidence (< 50%)')
    no_overtopping_recorded = mlines.Line2D([], [], color='black', marker='x', linestyle='None', markersize=10, label='No Overtopping')

    axes1_DG_Plot.set_title('Dawlish Rig 1', loc='center', fontsize=12, fontweight='bold')
    axes2_DG_Plot.set_title('Dawlish Rig 2', loc='center', fontsize=12, fontweight='bold')

    fig.legend(handles=[high_confidence_scoring_metrics, medium_confidence_scoring_metrics, low_confidence_scoring_metrics, no_overtopping_recorded], loc='lower center', bbox_to_anchor=(0.5, -0.15),
               ncol=4, frameon=False)

    plt.tight_layout()
    plt.show()


def on_submit_clicked(button):

    df_adjusted = adjust_features(final_dataset)
    clear_output(wait=True)
    process_wave_overtopping(df_adjusted)
    display(Sig_wave_height_slider_output, tm_slider, shore_wave_dir_slider,
            wind_speed_slider, shore_wind_dir_slider, freeboard_slider, submit_button)


process_wave_overtopping(final_dataset)
display(Sig_wave_height_slider_output, tm_slider, shore_wave_dir_slider,
        wind_speed_slider, shore_wind_dir_slider, freeboard_slider, submit_button)
submit_button.on_click(on_submit_clicked)